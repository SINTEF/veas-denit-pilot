{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(\n",
    "    search_from=os.getcwd(),\n",
    "    indicator=[\".git\", \"pyproject.toml\"],\n",
    "    pythonpath=True,\n",
    "    dotenv=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# %matplotlib notebook\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "import src.eval\n",
    "import src.utils\n",
    "import src.utils.plotting"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Inspect Iterative Experiment Results\n",
    "This notebook aids in inspecting the results of the iterative data chunks experiments, and in collecting the necessary information for the paper: gathering the figures and outputting latex table data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def search_mlflow(\n",
    "    search_experiment_name,\n",
    "    mlflow_tracking_uri=None,\n",
    "    mlflow_file_path=os.path.join(root, \"logs\", \"mlflow\", \"mlruns\"),\n",
    "):\n",
    "    tags_model_to_name = dict(\n",
    "        XGB=\"XGBoost\",\n",
    "        TCN=\"TCN\",\n",
    "        RNN=\"LSTM\",\n",
    "        Regression=\"ElasticNet\",\n",
    "        NaiveSeasonal=\"BaselineSeasonal\",\n",
    "        TCNNoTarget=\"TCN\",\n",
    "        RNNNoTarget=\"LSTM\",\n",
    "    )\n",
    "    if isinstance(search_experiment_name, str):\n",
    "        search_experiment_name = [search_experiment_name]\n",
    "\n",
    "    if mlflow_tracking_uri is None:\n",
    "        assert mlflow_file_path is not None\n",
    "        mlflow_tracking_uri = f\"file:///{mlflow_file_path}\"\n",
    "\n",
    "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "    df = mlflow.search_runs(experiment_names=search_experiment_name)\n",
    "    df[\"tags.model\"] = df[\"tags.model\"].apply(\n",
    "        lambda x: tags_model_to_name.get(x.replace(\"Model\", \"\"), x.replace(\"Model\", \"\"))\n",
    "    )\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "search_experiment_name = \"veas_pilot-nowcast-test\"\n",
    "latex_df = search_mlflow(\n",
    "    search_experiment_name\n",
    ")\n",
    "\n",
    "notation = \"%.2f\"\n",
    "base = 1\n",
    "latex_columns = [\"tags.model\", \"metrics.val_mse\", \"metrics.test_mse\", \"metrics.test_mae\"]\n",
    "\n",
    "aggregate = \"mean\"\n",
    "\n",
    "latex_df = latex_df[latex_df[\"status\"] == \"FINISHED\"]\n",
    "latex_df = latex_df[latex_columns]\n",
    "latex_df = latex_df.copy()\n",
    "\n",
    "# Calculate mean, median, and std for each group\n",
    "grouped = latex_df.groupby(\"tags.model\")\n",
    "latex_df_first = grouped.agg(aggregate).reset_index()\n",
    "latex_df_std = grouped.std().reset_index()\n",
    "\n",
    "# Handle NaN std values by replacing them with 0 (or you can choose to replace them with \"\")\n",
    "latex_df_std = latex_df_std.fillna(0)\n",
    "\n",
    "# Sort latex_df by 'metrics.test_mse'\n",
    "latex_df_first = latex_df_first.sort_values(by=\"metrics.test_mse\")\n",
    "\n",
    "# Reindex latex_df_std and latex_df_mean to match the sorted order of latex_df\n",
    "latex_df_std = latex_df_std.reindex(latex_df_first.index)\n",
    "\n",
    "# Combine median and std into the desired format\n",
    "for col in latex_columns[1:]:  # Skip the 'tags.model' column\n",
    "    latex_df_first[col] = latex_df_first[col].apply(lambda x: notation % x)\n",
    "    latex_df_std[col] = latex_df_std[col].apply(lambda x: notation % x)\n",
    "\n",
    "    # Use the mean only if std is 0, otherwise use the full format\n",
    "    latex_df_first[col] = latex_df_first[col] + latex_df_std[col].apply(\n",
    "        lambda std: \"\" if std == notation % 0 else r\" $\\pm$ \" + std\n",
    "    )\n",
    "latex_df_first[\"tags.model\"] = \"& \" + latex_df_first[\"tags.model\"]\n",
    "\n",
    "# Convert the dataframe to LaTeX format\n",
    "latex_output = latex_df_first.to_latex(index=False, float_format=notation)\n",
    "\n",
    "print(latex_output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lags Input Length History"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "src.utils.plotting.set_matplotlib_attributes(font_size=8)\n",
    "# Configurable metric for y-axis\n",
    "metric_name = \"test_mse\"\n",
    "metric_column = f\"metrics.{metric_name}\"\n",
    "metric_plot_name = \" \".join(metric_name.replace(\"test_\", \"\").split(\"_\")).upper()\n",
    "\n",
    "lags_column = \"params.model_lags\"\n",
    "lags_plot_name = \"Length of input (hours)\"\n",
    "\n",
    "model_name_column = \"tags.model\"\n",
    "\n",
    "models = [\"xgboost\", \"elastic_net\", \"rnn\", \"tcn\"]\n",
    "model_order = [\"ElasticNet\", \"LSTM\", \"TCN\", \"XGBoost\"]\n",
    "task = \"forecast\"\n",
    "\n",
    "search_experiment_name = [f\"veas_pilot-lags_test-veas_pilot_{model}_{task}\" for model in models]\n",
    "df = search_mlflow(search_experiment_name)\n",
    "\n",
    "df = df.loc[df[metric_column].notna()]\n",
    "\n",
    "\n",
    "# Rename columns for better plotting\n",
    "df = df.rename(columns={metric_column: metric_plot_name, lags_column: lags_plot_name})\n",
    "\n",
    "# Sort by 'chunk_length' numerically\n",
    "df[lags_plot_name] = df[lags_plot_name].astype(\n",
    "    int, errors=\"ignore\"\n",
    ")  # Convert to integer if it's not already\n",
    "df = df.sort_values(by=lags_plot_name)\n",
    "\n",
    "# Plotting\n",
    "plot = sns.lineplot(\n",
    "    data=df,\n",
    "    x=lags_plot_name,\n",
    "    y=metric_plot_name,\n",
    "    hue=model_name_column,\n",
    "    # marker=\"o\",\n",
    "    hue_order=model_order,\n",
    ")\n",
    "# set_figure_size(plot.get_figure(), column_span=fig_column_span, height=fig_height)\n",
    "\n",
    "# Set x-ticks to only where there is data\n",
    "unique_lags_lengths = df[lags_plot_name].unique()[::4]\n",
    "plot.set_xticks(unique_lags_lengths)\n",
    "\n",
    "# Convert x-tick labels from count of 10 minutes to hours\n",
    "plot.set_xticklabels([f\"{length / 6:.0f}\" for length in unique_lags_lengths])\n",
    "\n",
    "if task == \"forecast\":\n",
    "    # Remove the legend title\n",
    "    legend = plot.legend_\n",
    "    legend.set_title(\"\")\n",
    "    legend.set_frame_on(False)\n",
    "else:\n",
    "    plt.legend([])\n",
    "\n",
    "plot.set_title(task.capitalize())\n",
    "\n",
    "fig_folder_name = \"veas_pilot\"\n",
    "# fig_folder_name = \"-\".join(search_experiment_name[0].split(\"-\")[:-1])\n",
    "fig_path = os.path.join(root, \"figures\", fig_folder_name, f\"{task}_lags\")\n",
    "src.utils.plotting.set_figure_size(plot.figure, \"single\", height=6)\n",
    "src.utils.plotting.save_figure(plot.figure, fig_path)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot model outputs for paper"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_objects(model_dir, dot_overrides=None):\n",
    "    model_dir = src.utils.hydra.get_absolute_project_path(model_dir)\n",
    "    config_path = os.path.join(\n",
    "        \"..\", \"..\", \"configs\", \"eval.yaml\"\n",
    "    )  # NB: relative to <project_root>/src/utils (must be relative path)\n",
    "\n",
    "    config_overrides_dot = [  # same notation as for cli overrides (dot notation). Useful for changing whole modules, e.g. change which datamodule file is loaded\n",
    "        \"++extras.disable_pytorch_lightning_output=True\",\n",
    "        \"++extras.select_gpu=False\",\n",
    "        \"++extras.matplotlib_backend=null\",\n",
    "    ]\n",
    "    if dot_overrides is not None:\n",
    "        config_overrides_dot.extend(dot_overrides)\n",
    "    config_overrides_dict = {\n",
    "        \"model_dir\": model_dir,\n",
    "        \"datamodule\": dict(train_val_test_split=dict(test=[\"2024-01-29 10:30:00\", \"2024-02-28\"])),\n",
    "        \"eval\": dict(\n",
    "            split=\"test\",\n",
    "            plot=False,\n",
    "            predictions={\"return\": {\"data\": True}},\n",
    "            show_warnings=False,\n",
    "            kwargs=dict(metric=None),\n",
    "        ),\n",
    "    }  # Dictionary with overrides. Useful for larger changes/additions/deletions that does not exist as entire files.\n",
    "\n",
    "    cfg = src.utils.initialize_hydra(\n",
    "        config_path,\n",
    "        config_overrides_dot,\n",
    "        config_overrides_dict,\n",
    "        return_hydra_config=True,\n",
    "        print_config=False,\n",
    "    )  # print config to inspect if all settings are as expected\n",
    "    with open_dict(cfg):\n",
    "        cfg.logger = None\n",
    "\n",
    "    objects = src.utils.instantiate.instantiate_saved_objects(cfg)\n",
    "    objects[\"cfg\"] = cfg\n",
    "\n",
    "    return objects"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_dirs = {\n",
    "    \"forecast\": \"logs/train/multiruns/2024-09-04_16-27-29/20\",\n",
    "    \"nowcast\": \"logs/train/multiruns/2024-09-03_20-02-19/14\",\n",
    "}\n",
    "\n",
    "objects = {\n",
    "    \"forecast\": load_objects(model_dirs[\"forecast\"], [\"experiment=veas_pilot_forecast_test\"]),\n",
    "    \"nowcast\": load_objects(model_dirs[\"nowcast\"], [\"experiment=veas_pilot_nowcast_test\"]),\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "eval_results = {}\n",
    "for task, object_dict in objects.items():\n",
    "    metric_dict, eval_object_dict = src.eval.run(\n",
    "        object_dict[\"cfg\"],\n",
    "        object_dict[\"datamodule\"],\n",
    "        object_dict[\"model\"],\n",
    "        object_dict.get(\"trainer\", None),\n",
    "        object_dict.get(\"logger\", None),\n",
    "    )\n",
    "    eval_results[task] = eval_object_dict\n",
    "    eval_results[task][\"metrics\"] = metric_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_period(eval_results, start_time, end_time, every_n_predictions=3):\n",
    "    # Plot the forecast lines with different colors\n",
    "    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "    src.utils.plotting.set_matplotlib_attributes()\n",
    "\n",
    "    fig = src.utils.plotting.plot_prediction(\n",
    "        [\n",
    "            p\n",
    "            for p_i, p in enumerate(eval_results[\"forecast\"][\"predictions\"])\n",
    "            if p_i % every_n_predictions == 0 and start_time <= p.start_time() <= end_time\n",
    "        ],\n",
    "        eval_results[\"forecast\"][\"predictions_data\"],\n",
    "        objects[\"forecast\"][\"model\"],\n",
    "        None,\n",
    "        plot_covariates=False,\n",
    "        plot_past=False,\n",
    "    )\n",
    "    fig = fig[0]\n",
    "    ax = fig.axes[0]\n",
    "\n",
    "    eval_results[\"nowcast\"][\"predictions\"].slice(start_time, end_time).plot(\n",
    "        ax=ax, color=\"orange\", linestyle=\"dashed\", label=\"Nowcast\"\n",
    "    )\n",
    "\n",
    "    for line_i in range(len(ax.lines)):\n",
    "        ax.lines[line_i].set_linewidth(1)\n",
    "    # eval_object_dict[\"predictions_data\"][\"series\"].plot(label=\"_nolegend_\")\n",
    "    # eval_object_dict[\"predictions\"].plot(label=\"_nolegend_\")\n",
    "\n",
    "    ax.set_ylabel(\"Nitrate concentration [mg/l]\")\n",
    "    ax.set_title(\"Nowcast and hour-ahead forecasts from LSTM\")\n",
    "    src.utils.plotting.set_figure_size(fig, column_span=\"double\", height=8)\n",
    "    src.utils.plotting.save_figure(fig, \"../figures/pilot/predictions_lstm\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_period(eval_results, pd.Timestamp(\"2024-02-13 12:00:00\"), pd.Timestamp(\"2024-02-14 12:00:00\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_period(eval_results, pd.Timestamp(\"2024-02-14 12:00:00\"), pd.Timestamp(\"2024-02-17 12:00:00\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import darts.metrics\n",
    "\n",
    "metrics = {\"forecast\": [], \"nowcast\": []}\n",
    "\n",
    "for p_i, p in enumerate(eval_results[\"forecast\"][\"predictions\"]):\n",
    "    metrics[\"forecast\"].append(\n",
    "        darts.metrics.mse(\n",
    "            eval_results[\"forecast\"][\"predictions_data\"][\"series\"], p, intersect=True\n",
    "        )\n",
    "    )\n",
    "    metrics[\"nowcast\"].append(\n",
    "        darts.metrics.mse(\n",
    "            eval_results[\"forecast\"][\"predictions_data\"][\"series\"],\n",
    "            eval_results[\"nowcast\"][\"predictions\"][p.start_time()],\n",
    "            intersect=True,\n",
    "        )\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "ax.plot(\n",
    "    eval_results[\"nowcast\"][\"predictions\"].time_index[: len(metrics[\"forecast\"])],\n",
    "    metrics[\"forecast\"],\n",
    "    label=\"forecast\",\n",
    ")\n",
    "ax.plot(\n",
    "    eval_results[\"nowcast\"][\"predictions\"].time_index[: len(metrics[\"forecast\"])],\n",
    "    metrics[\"nowcast\"],\n",
    "    label=\"nowcast\",\n",
    ")\n",
    "plt.legend()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
